{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f875eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "from tqdm import trange,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcc0d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1  -1  -1 100]\n",
      " [ -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1]]\n"
     ]
    }
   ],
   "source": [
    "# Constantes \n",
    "GOAL_SCORE = 100.0\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.95\n",
    "EPSILON = 0.8\n",
    "EPOCHS = 10000\n",
    "\n",
    "# Variables\n",
    "grid_rows = 3\n",
    "grid_cols = 4\n",
    "\n",
    "q_table = np.zeros((grid_rows, grid_cols, 4)) \n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "rewards = np.full((grid_rows, grid_cols), -1)\n",
    "rewards[0, 3] = GOAL_SCORE # meta\n",
    "obstacles = [[1,1]] # obstaculo\n",
    "\n",
    "print(rewards)\n",
    "\n",
    "\n",
    "# 1.b\n",
    "q_table_b = np.zeros((grid_rows, grid_cols, 4))\n",
    "rewards_b = np.array([\n",
    "    [-3., -2., -1., GOAL_SCORE],\n",
    "    [-4., -100., -2., -1.],\n",
    "    [-5., -4., -3., -2.]\n",
    "])\n",
    "rewards_b[0, 3] = GOAL_SCORE\n",
    "obstacles_b = [[1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf797a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones \n",
    "def is_terminal_state(row, col, rewards=rewards):\n",
    "    if rewards[row, col] == GOAL_SCORE:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_random_location():\n",
    "    row = np.random.randint(grid_rows)\n",
    "    col = np.random.randint(grid_cols)\n",
    "    while is_terminal_state(row, col) or [row, col] in obstacles:\n",
    "        row = np.random.randint(grid_rows)\n",
    "        col = np.random.randint(grid_cols)\n",
    "    return row, col\n",
    "\n",
    "def get_next_action(row, col, q_table=q_table, epsilon=EPSILON):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    return np.argmax(q_table[row, col])\n",
    "\n",
    "def get_next_state(row, col, q_table=q_table, obstacles=obstacles, epsilon=EPSILON):\n",
    "    finded = False\n",
    "    while not finded:\n",
    "        action_index = get_next_action(row, col, q_table=q_table, epsilon=epsilon)\n",
    "        action = actions[action_index]\n",
    "        new_row, new_col = row, col\n",
    "        if action == 'up' and row > 0:\n",
    "            new_row -= 1\n",
    "        elif action == 'down' and row < grid_rows - 1:\n",
    "            new_row += 1\n",
    "        elif action == 'left' and col > 0:\n",
    "            new_col -= 1\n",
    "        elif action == 'right' and col < grid_cols - 1:\n",
    "            new_col += 1\n",
    "\n",
    "        if [new_row, new_col] not in obstacles and (new_row != row or new_col != col):\n",
    "            finded = True\n",
    "    return new_row, new_col, action_index\n",
    "\n",
    "def get_shortest_path(start_row, start_col, q_table=q_table, rewards=rewards, obstacles=obstacles):\n",
    "    if is_terminal_state(start_row, start_col, rewards=rewards):\n",
    "        return []\n",
    "    else:\n",
    "        current_row, current_col = start_row, start_col\n",
    "        path = [[current_row, current_col]]\n",
    "        while not is_terminal_state(current_row, current_col, rewards=rewards):\n",
    "            current_row, current_col, _ = get_next_state(current_row, current_col, q_table=q_table, obstacles=obstacles, epsilon=0.0)\n",
    "            path.append([current_row, current_col])\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecucion\n",
    "\n",
    "def train_delta(q_table=q_table, grid=rewards, obstacles=obstacles, conv_threshold=0.001, patience=20):\n",
    "    stable_epochs = 0\n",
    "    for epoch in trange(EPOCHS):\n",
    "        next_row, next_col = 2, 0\n",
    "        old_matrix = q_table.copy()\n",
    "        while not is_terminal_state(next_row, next_col, rewards):\n",
    "            old_row, old_col = next_row, next_col\n",
    "\n",
    "            next_row, next_col, action_index = get_next_state(next_row, next_col, q_table=q_table, obstacles=obstacles)\n",
    "\n",
    "            next_reward = grid[next_row, next_col]\n",
    "            old_q_value = q_table[old_row, old_col, action_index]\n",
    "\n",
    "            temporal_difference = next_reward + (GAMMA * np.max(q_table[next_row, next_col])) - old_q_value\n",
    "            new_q_value = old_q_value + (ALPHA * temporal_difference)\n",
    "\n",
    "            q_table[old_row, old_col, action_index] = new_q_value\n",
    "\n",
    "        delta = np.linalg.norm(q_table - old_matrix)\n",
    "        if delta < conv_threshold:\n",
    "            stable_epochs += 1\n",
    "            if stable_epochs >= patience:\n",
    "                print(f\"Converged after {epoch+1} epochs.\")\n",
    "                break\n",
    "        else:\n",
    "            stable_epochs = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "2f471e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_std(q_table, grid, obstacles, \n",
    "          alpha = ALPHA, gamma = GAMMA, \n",
    "          epsilon = EPSILON, epsilon_min = 0.01, epsilon_decay = 0.995,\n",
    "          conv_threshold = 0.1, window_size = 10):\n",
    "    \n",
    "    snapshots = [q_table.copy()]\n",
    "    reward_history = []\n",
    "\n",
    "    pbar = tqdm(desc=\"Training\")\n",
    "    epoch_limit = 100000\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch < epoch_limit:\n",
    "      epoch += 1\n",
    "      pbar.update(1)\n",
    "\n",
    "      next_row, next_col = 2, 0\n",
    "      total_reward = 0\n",
    "      while not is_terminal_state(next_row, next_col, grid):\n",
    "        old_row, old_col = next_row, next_col\n",
    "\n",
    "        next_row, next_col, action_index = get_next_state(next_row, next_col, q_table=q_table, obstacles=obstacles, epsilon=epsilon)\n",
    "\n",
    "        next_reward = grid[next_row, next_col]\n",
    "        old_q_value = q_table[old_row, old_col, action_index]\n",
    "\n",
    "        temporal_difference = next_reward + (gamma * np.max(q_table[next_row, next_col])) - old_q_value\n",
    "        new_q_value = old_q_value + (alpha * temporal_difference)\n",
    "\n",
    "        q_table[old_row, old_col, action_index] = new_q_value\n",
    "\n",
    "        total_reward += next_reward\n",
    "\n",
    "      reward_history.append(total_reward)\n",
    "      epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "      snapshots.append(q_table.copy())\n",
    "\n",
    "      if len(reward_history) >= window_size:\n",
    "         window = reward_history[-window_size:]\n",
    "         if np.std(window) < conv_threshold:\n",
    "            print(\"Convergio en el epoch: \", epoch)\n",
    "            pbar.close()\n",
    "            break\n",
    "         \n",
    "    pbar.close()\n",
    "    snapshots.append(q_table.copy())\n",
    "    snapshots_output = [\n",
    "       snapshots[0],\n",
    "       snapshots[int(len(snapshots)*0.33)],\n",
    "       snapshots[int(len(snapshots)*0.66)],\n",
    "       snapshots[-1]\n",
    "    ]\n",
    "\n",
    "    return snapshots_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q_table, grid, obstacles, \n",
    "          alpha = ALPHA, gamma = GAMMA, \n",
    "          epsilon = EPSILON, epsilon_min = 0.01, epsilon_decay = 0.995,\n",
    "          conv_threshold = [0.1, 0.1], window_size = 10, patience=10):\n",
    "   \n",
    "    \n",
    "   snapshots = [q_table.copy()]\n",
    "   reward_history = []\n",
    "\n",
    "   epoch_limit = 30000\n",
    "   total_epochs = 0\n",
    "   stable_epochs = 0\n",
    "   is_converged = False\n",
    "\n",
    "   for epoch in range(epoch_limit):\n",
    "      total_epochs += 1\n",
    "\n",
    "      next_row, next_col = 2, 0\n",
    "      total_reward = 0\n",
    "      old_q_table = q_table.copy()\n",
    "\n",
    "      while not is_terminal_state(next_row, next_col, grid):\n",
    "         old_row, old_col = next_row, next_col\n",
    "\n",
    "         next_row, next_col, action_index = get_next_state(next_row, next_col, q_table=q_table, obstacles=obstacles, epsilon=epsilon)\n",
    "\n",
    "         next_reward = grid[next_row, next_col]\n",
    "         old_q_value = q_table[old_row, old_col, action_index]\n",
    "\n",
    "         temporal_difference = next_reward + (gamma * np.max(q_table[next_row, next_col])) - old_q_value\n",
    "         new_q_value = old_q_value + (alpha * temporal_difference)\n",
    "\n",
    "         q_table[old_row, old_col, action_index] = new_q_value\n",
    "\n",
    "         total_reward += next_reward\n",
    "\n",
    "      delta = np.linalg.norm(q_table - old_q_table)\n",
    "      reward_history.append(total_reward)\n",
    "      epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "      snapshots.append(q_table.copy())\n",
    "\n",
    "      if len(reward_history) >= window_size:\n",
    "         window = reward_history[-window_size:]\n",
    "         std_window = np.std(window)\n",
    "      else:\n",
    "         std_window = float('inf')\n",
    "\n",
    "      if delta < conv_threshold[0] and std_window < conv_threshold[1]:\n",
    "            stable_epochs += 1\n",
    "            if stable_epochs >= patience:\n",
    "               #print(\"Convergio en el epoch: \", total_epochs)\n",
    "               is_converged = True\n",
    "               break\n",
    "      else:\n",
    "            stable_epochs = 0\n",
    "         \n",
    "   if not np.array_equal(q_table, snapshots[-1]):\n",
    "      snapshots.append(q_table.copy())\n",
    "\n",
    "   snapshots_output = [\n",
    "      snapshots[0],\n",
    "      snapshots[int(len(snapshots)*0.33)],\n",
    "      snapshots[int(len(snapshots)*0.66)],\n",
    "      snapshots[-1]\n",
    "   ]\n",
    "\n",
    "   avg_reward_final = np.mean(reward_history[-window_size:]) if len(reward_history) >= window_size else np.mean(reward_history)\n",
    "   avg_reward_total = np.mean(reward_history)\n",
    "   avg_reward_100 = np.mean(reward_history[-100:]) if len(reward_history) >= 100 else np.mean(reward_history)\n",
    "\n",
    "   results_dict = {\n",
    "      \"snapshots\": snapshots_output,\n",
    "      \"avg_reward_final\": avg_reward_final,\n",
    "      \"avg_reward_total\": avg_reward_total,\n",
    "      \"avg_reward_100\": avg_reward_100,\n",
    "      \"total_epochs\": total_epochs,\n",
    "      \"is_converged\": is_converged\n",
    "   }\n",
    "\n",
    "   return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d7660c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 467/100000 [00:00<00:20, 4824.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergio en el epoch:  468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((grid_rows, grid_cols, 4)) \n",
    "#output = train_std(q_table=q_table, grid=rewards, obstacles=obstacles, conv_threshold=0.00001, window_size=50)\n",
    "#train(q_table,rewards,obstacles, conv_threshold=0.1, patience=20)\n",
    "output = train(q_table=q_table, grid=rewards, obstacles=obstacles, conv_threshold=[0.0001,0.01], window_size=20, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4c5fcae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.        ,   9.39576234,   0.        ,  20.98875995],\n",
       "        [  0.        ,   0.        ,   0.51227339,  63.74672578],\n",
       "        [  0.        ,  62.26216957,  20.5132619 ,  99.12720364],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[  5.87607678,  72.16613654,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [ 89.41038685,  79.83469657,   0.        ,  94.        ],\n",
       "        [100.        ,  85.89523165,  87.71817715,   0.        ]],\n",
       "\n",
       "       [[ 64.88483149,   0.        ,   0.        ,  77.74075   ],\n",
       "        [  0.        ,   0.        ,  71.91713975,  82.885     ],\n",
       "        [ 88.3       ,   0.        ,  76.62256644,  86.81153301],\n",
       "        [ 93.9687413 ,   0.        ,  64.34045335,   0.        ]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[2, 0], [2, 1], [2, 2], [1, 2], [1, 3], [0, 3]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avr si jala esta vaina \n",
    "display(q_table)\n",
    "get_shortest_path(2,0, q_table, rewards, obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9abff24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 471/100000 [00:00<00:20, 4900.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergio en el epoch:  472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_table_b = np.zeros((grid_rows, grid_cols, 4))\n",
    "#output =train_delta(q_table=q_table_b, grid=rewards_b, obstacles=obstacles_b, conv_threshold=0.001, patience=20)\n",
    "output =train(q_table=q_table_b, grid=rewards_b, obstacles=obstacles_b, conv_threshold=[0.0001,0.01], window_size=20, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "36eca701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.        ,  71.08676633,   0.        ,  87.3       ],\n",
       "        [  0.        ,   0.        ,  79.06725218,  94.        ],\n",
       "        [  0.        ,  80.54491417,  86.08450391, 100.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
       "\n",
       "       [[ 79.935     ,  61.43842673,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [ 93.85215593,  40.86049533,   0.        ,  23.99911309],\n",
       "        [ 74.58134172,   1.05759267,  14.14913477,   0.        ]],\n",
       "\n",
       "       [[ 71.93825   ,   0.        ,   0.        ,  48.11605725],\n",
       "        [  0.        ,   0.        ,  17.43025647,  69.69861071],\n",
       "        [ 84.99264483,   0.        ,   2.64089184,   7.09764575],\n",
       "        [ 29.91133017,   0.        ,   1.96343993,   0.        ]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[2, 0], [1, 0], [0, 0], [0, 1], [0, 2], [0, 3]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(q_table_b)\n",
    "get_shortest_path(2,0, q_table_b, rewards_b, obstacles_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cbaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 172,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 99/172800 [00:11<5:35:50,  8.57it/s]\n",
      "Testing:   1%|          | 919/172800 [01:31<4:34:15, 10.45it/s] "
     ]
    }
   ],
   "source": [
    "# Grid search parameters\n",
    "test_alphas = [0.05, 0.1, 0.2, 0.3]\n",
    "test_gammas = [0.8, 0.9, 0.95, 0.99]\n",
    "test_epsilons = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "test_windows = [10, 30, 50]\n",
    "test_conv_stds = [0.5, 0.3, 0.1, 0.01]\n",
    "test_conv_deltas = [0.1, 0.01, 0.001, 0.00001]\n",
    "test_patiences = [10, 30, 50]\n",
    "test_decays = [0.99, 0.995, 0.997]\n",
    "test_num_runs = 5\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = (len(test_alphas) * len(test_gammas) * len(test_epsilons) * \n",
    "                      len(test_windows) * len(test_conv_stds) * len(test_conv_deltas) * \n",
    "                      len(test_patiences) * len(test_decays) * test_num_runs)\n",
    "print(f\"Total experiments: {total_combinations:,}\")\n",
    "\n",
    "results = []\n",
    "pbar = tqdm(total=total_combinations, desc=\"Testing\")\n",
    "for alpha in test_alphas:\n",
    "    for gamma in test_gammas:\n",
    "        for epsilon in test_epsilons:\n",
    "            for window in test_windows:\n",
    "                for conv_std in test_conv_stds:\n",
    "                    for conv_delta in test_conv_deltas:\n",
    "                        for patience in test_patiences:\n",
    "                            for decay in test_decays:\n",
    "                                run_results = []\n",
    "                                for _ in range(test_num_runs):\n",
    "                                    q_table = np.zeros((grid_rows, grid_cols, 4))\n",
    "                                    result = train(q_table=q_table, grid=rewards, obstacles=obstacles,\n",
    "                                          alpha=alpha, gamma=gamma, epsilon=epsilon, epsilon_decay=decay,\n",
    "                                          conv_threshold=[conv_delta, conv_std], window_size=window, patience=patience)\n",
    "                                    run_results.append({\n",
    "                                        'is_converged': result['is_converged'],\n",
    "                                        'convergence_epochs': result['total_epochs'],\n",
    "                                        'path_length': len(get_shortest_path(2, 0, q_table, rewards, obstacles)),\n",
    "                                        'avg_reward_total': result['avg_reward_total'],\n",
    "                                        'avg_reward_final': result['avg_reward_final'],\n",
    "                                        'avg_reward_100': result['avg_reward_100']\n",
    "                                    })\n",
    "                                    pbar.update(1)\n",
    "\n",
    "                                # Aggregate results for this configuration\n",
    "                                avg_convergence_epochs = np.mean([r['convergence_epochs'] for r in run_results if r['is_converged']]) if any(r['is_converged'] for r in run_results) else None\n",
    "                                avg_path_length = np.mean([r['path_length'] for r in run_results])\n",
    "                                avg_reward_total = np.mean([r['avg_reward_total'] for r in run_results])\n",
    "                                avg_reward_final = np.mean([r['avg_reward_final'] for r in run_results])\n",
    "                                avg_reward_100 = np.mean([r['avg_reward_100'] for r in run_results])\n",
    "                                convergence_rate = sum(r['is_converged'] for r in run_results) / test_num_runs\n",
    "\n",
    "                                results.append({\n",
    "                                    'alpha': alpha,\n",
    "                                    'gamma': gamma,\n",
    "                                    'epsilon': epsilon,\n",
    "                                    'window_size': window,\n",
    "                                    'conv_std': conv_std,\n",
    "                                    'conv_delta': conv_delta,\n",
    "                                    'patience': patience,\n",
    "                                    'decay': decay,\n",
    "                                    'avg_convergence_epochs': avg_convergence_epochs,\n",
    "                                    'avg_path_length': avg_path_length,\n",
    "                                    'avg_reward_total': avg_reward_total,\n",
    "                                    'avg_reward_final': avg_reward_final,\n",
    "                                    'avg_reward_100': avg_reward_100,\n",
    "                                    'convergence_rate': convergence_rate\n",
    "                                })\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nCompleted {len(results)} unique configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# AN√ÅLISIS DE RESULTADOS\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_results(df):\n",
    "    \"\"\"\n",
    "    Analiza y visualiza los resultados del grid search\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä AN√ÅLISIS DE RESULTADOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Top 5 configuraciones por velocidad de convergencia\n",
    "    print(\"\\nüèÜ Top 5: Convergencia m√°s r√°pida\")\n",
    "    top_speed = df[df['avg_convergence_epochs'].notna()].nsmallest(5, 'avg_convergence_epochs')[\n",
    "        ['alpha', 'gamma', 'epsilon', 'window_size', 'avg_convergence_epochs', 'convergence_rate']\n",
    "    ]\n",
    "    print(top_speed.to_string(index=False))\n",
    "    \n",
    "    # Top 5 configuraciones por longitud de camino\n",
    "    print(\"\\nüéØ Top 5: Camino m√°s corto\")\n",
    "    top_path = df.nsmallest(5, 'avg_path_length')[\n",
    "        ['alpha', 'gamma', 'epsilon', 'avg_path_length', 'avg_reward_final', 'convergence_rate']\n",
    "    ]\n",
    "    print(top_path.to_string(index=False))\n",
    "    \n",
    "    # Top 5 por recompensa final promedio\n",
    "    print(\"\\nüí∞ Top 5: Mayor recompensa final promedio\")\n",
    "    top_reward = df.nlargest(5, 'avg_reward_final')[\n",
    "        ['alpha', 'gamma', 'epsilon', 'avg_reward_final', 'avg_path_length', 'convergence_rate']\n",
    "    ]\n",
    "    print(top_reward.to_string(index=False))\n",
    "    \n",
    "    # Top 5 por recompensa √∫ltimos 100 epochs\n",
    "    print(\"\\nüìà Top 5: Mayor recompensa (√∫ltimos 100 epochs)\")\n",
    "    top_reward_100 = df.nlargest(5, 'avg_reward_100')[\n",
    "        ['alpha', 'gamma', 'epsilon', 'avg_reward_100', 'avg_path_length', 'convergence_rate']\n",
    "    ]\n",
    "    print(top_reward_100.to_string(index=False))\n",
    "    \n",
    "    # Configuraciones que NO convergieron\n",
    "    not_converged = df[df['convergence_rate'] < 1.0]\n",
    "    if len(not_converged) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(not_converged)} configuraciones con problemas de convergencia\")\n",
    "    \n",
    "    # Mejor configuraci√≥n balanceada\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üåü MEJOR CONFIGURACI√ìN BALANCEADA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Normalizar m√©tricas para scoring\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Solo considerar filas que convergieron para el score de convergencia\n",
    "    converged_mask = df['avg_convergence_epochs'].notna()\n",
    "    if converged_mask.any():\n",
    "        max_epochs = df.loc[converged_mask, 'avg_convergence_epochs'].max()\n",
    "        df_norm.loc[converged_mask, 'score_convergence'] = 1 - (df.loc[converged_mask, 'avg_convergence_epochs'] / max_epochs)\n",
    "        df_norm.loc[~converged_mask, 'score_convergence'] = 0\n",
    "    else:\n",
    "        df_norm['score_convergence'] = 0\n",
    "    \n",
    "    df_norm['score_path'] = 1 - (df['avg_path_length'] / df['avg_path_length'].max())\n",
    "    df_norm['score_reward_final'] = (df['avg_reward_final'] - df['avg_reward_final'].min()) / (df['avg_reward_final'].max() - df['avg_reward_final'].min())\n",
    "    df_norm['score_reward_100'] = (df['avg_reward_100'] - df['avg_reward_100'].min()) / (df['avg_reward_100'].max() - df['avg_reward_100'].min())\n",
    "    \n",
    "    # Score balanceado (puedes ajustar los pesos)\n",
    "    df_norm['balanced_score'] = (\n",
    "        0.2 * df_norm['score_convergence'] +     # 20% velocidad\n",
    "        0.4 * df_norm['score_path'] +             # 40% calidad del camino\n",
    "        0.2 * df_norm['score_reward_final'] +     # 20% recompensa final\n",
    "        0.2 * df_norm['score_reward_100']         # 20% recompensa √∫ltimos 100\n",
    "    ) * df_norm['convergence_rate']  # Penalizar si no converge siempre\n",
    "    \n",
    "    best = df_norm.nlargest(1, 'balanced_score').iloc[0]\n",
    "    print(f\"\\nAlpha:       {best['alpha']}\")\n",
    "    print(f\"Gamma:       {best['gamma']}\")\n",
    "    print(f\"Epsilon:     {best['epsilon']}\")\n",
    "    print(f\"Window size: {best['window_size']}\")\n",
    "    print(f\"Conv std:    {best['conv_std']}\")\n",
    "    print(f\"Conv delta:  {best['conv_delta']}\")\n",
    "    print(f\"Patience:    {best['patience']}\")\n",
    "    print(f\"Decay:       {best['decay']}\")\n",
    "    print(f\"\\nM√©tricas:\")\n",
    "    if pd.notna(best['avg_convergence_epochs']):\n",
    "        print(f\"  - Convergencia:      {best['avg_convergence_epochs']:.0f} epochs\")\n",
    "    else:\n",
    "        print(f\"  - Convergencia:      No convergi√≥\")\n",
    "    print(f\"  - Path length:       {best['avg_path_length']:.1f} pasos\")\n",
    "    print(f\"  - Recompensa final:  {best['avg_reward_final']:.2f}\")\n",
    "    print(f\"  - Recompensa total:  {best['avg_reward_total']:.2f}\")\n",
    "    print(f\"  - Recompensa (100):  {best['avg_reward_100']:.2f}\")\n",
    "    print(f\"  - Tasa convergencia: {best['convergence_rate']*100:.0f}%\")\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZACI√ìN\n",
    "# ============================================\n",
    "def plot_parameter_effects(df):\n",
    "    \"\"\"\n",
    "    Grafica el efecto de cada par√°metro\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Efecto de Alpha\n",
    "    alpha_effect = df.groupby('alpha').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'avg_path_length': 'mean',\n",
    "        'avg_reward_final': 'mean'\n",
    "    })\n",
    "    axes[0].plot(alpha_effect.index, alpha_effect['avg_convergence_epochs'], 'o-')\n",
    "    axes[0].set_xlabel('Alpha (learning rate)')\n",
    "    axes[0].set_ylabel('Epochs to converge')\n",
    "    axes[0].set_title('Efecto de Alpha en Convergencia')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Efecto de Gamma\n",
    "    gamma_effect = df.groupby('gamma').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'avg_path_length': 'mean',\n",
    "        'avg_reward_final': 'mean'\n",
    "    })\n",
    "    axes[1].plot(gamma_effect.index, gamma_effect['avg_path_length'], 'o-', color='orange')\n",
    "    axes[1].set_xlabel('Gamma (discount factor)')\n",
    "    axes[1].set_ylabel('Path length')\n",
    "    axes[1].set_title('Efecto de Gamma en Path Length')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Efecto de Epsilon\n",
    "    epsilon_effect = df.groupby('epsilon').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'avg_path_length': 'mean',\n",
    "        'avg_reward_final': 'mean'\n",
    "    })\n",
    "    axes[2].plot(epsilon_effect.index, epsilon_effect['avg_reward_final'], 'o-', color='green')\n",
    "    axes[2].set_xlabel('Epsilon (exploration rate)')\n",
    "    axes[2].set_ylabel('Average reward (final)')\n",
    "    axes[2].set_title('Efecto de Epsilon en Recompensa')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    # Efecto de Window Size\n",
    "    window_effect = df.groupby('window_size').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'convergence_rate': 'mean'\n",
    "    })\n",
    "    axes[3].plot(window_effect.index, window_effect['convergence_rate'], 'o-', color='purple')\n",
    "    axes[3].set_xlabel('Window Size')\n",
    "    axes[3].set_ylabel('Convergence Rate')\n",
    "    axes[3].set_title('Efecto de Window Size')\n",
    "    axes[3].grid(True)\n",
    "    \n",
    "    # Efecto de Conv Std\n",
    "    conv_std_effect = df.groupby('conv_std').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'convergence_rate': 'mean'\n",
    "    })\n",
    "    axes[4].plot(conv_std_effect.index, conv_std_effect['avg_convergence_epochs'], 'o-', color='red')\n",
    "    axes[4].set_xlabel('Conv Std Threshold')\n",
    "    axes[4].set_ylabel('Epochs to converge')\n",
    "    axes[4].set_title('Efecto de Conv Std')\n",
    "    axes[4].set_xscale('log')\n",
    "    axes[4].grid(True)\n",
    "    \n",
    "    # Efecto de Conv Delta\n",
    "    conv_delta_effect = df.groupby('conv_delta').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'convergence_rate': 'mean'\n",
    "    })\n",
    "    axes[5].plot(conv_delta_effect.index, conv_delta_effect['avg_convergence_epochs'], 'o-', color='brown')\n",
    "    axes[5].set_xlabel('Conv Delta Threshold')\n",
    "    axes[5].set_ylabel('Epochs to converge')\n",
    "    axes[5].set_title('Efecto de Conv Delta')\n",
    "    axes[5].set_xscale('log')\n",
    "    axes[5].grid(True)\n",
    "    \n",
    "    # Efecto de Patience\n",
    "    patience_effect = df.groupby('patience').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'convergence_rate': 'mean'\n",
    "    })\n",
    "    axes[6].plot(patience_effect.index, patience_effect['convergence_rate'], 'o-', color='pink')\n",
    "    axes[6].set_xlabel('Patience')\n",
    "    axes[6].set_ylabel('Convergence Rate')\n",
    "    axes[6].set_title('Efecto de Patience')\n",
    "    axes[6].grid(True)\n",
    "    \n",
    "    # Efecto de Decay\n",
    "    decay_effect = df.groupby('decay').agg({\n",
    "        'avg_convergence_epochs': 'mean',\n",
    "        'avg_reward_final': 'mean'\n",
    "    })\n",
    "    axes[7].plot(decay_effect.index, decay_effect['avg_reward_final'], 'o-', color='cyan')\n",
    "    axes[7].set_xlabel('Epsilon Decay')\n",
    "    axes[7].set_ylabel('Average Reward (final)')\n",
    "    axes[7].set_title('Efecto de Epsilon Decay')\n",
    "    axes[7].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Convertir results a DataFrame y analizar\n",
    "df_results = pd.DataFrame(results)\n",
    "df_analyzed = analyze_results(df_results)\n",
    "plot_parameter_effects(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
