{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f875eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "from tqdm import trange,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcc0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes \n",
    "GOAL_SCORE = 100.0\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.95\n",
    "EPSILON = 0.8\n",
    "EPOCHS = 10000\n",
    "\n",
    "# Variables\n",
    "grid_rows = 3\n",
    "grid_cols = 4\n",
    "\n",
    "q_table = np.zeros((grid_rows, grid_cols, 4)) \n",
    "\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "\n",
    "rewards = np.full((grid_rows, grid_cols), -1)\n",
    "rewards[0, 3] = GOAL_SCORE # meta\n",
    "obstacles = [[1,1]] # obstaculo\n",
    "\n",
    "# 1.b\n",
    "q_table_b = np.zeros((grid_rows, grid_cols, 4))\n",
    "rewards_b = np.array([\n",
    "    [-3., -2., -1., GOAL_SCORE],\n",
    "    [-4., -100., -2., -1.],\n",
    "    [-5., -4., -3., -2.]\n",
    "])\n",
    "rewards_b[0, 3] = GOAL_SCORE\n",
    "obstacles_b = [[1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf797a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones del Entorno\n",
    "\n",
    "def is_terminal_state(row, col, grid_rewards):\n",
    "    \"\"\"Verifica si el estado actual es terminal (meta).\"\"\"\n",
    "    if grid_rewards[row, col] == GOAL_SCORE:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_random_location(grid_rows, grid_cols, obstacles, grid_rewards):\n",
    "    \"\"\"Obtiene una ubicación aleatoria válida que no sea terminal ni obstáculo.\"\"\"\n",
    "    while True:\n",
    "        row = np.random.randint(grid_rows)\n",
    "        col = np.random.randint(grid_cols)\n",
    "        if not is_terminal_state(row, col, grid_rewards) and [row, col] not in obstacles:\n",
    "            return row, col\n",
    "\n",
    "def get_next_action(row, col, q_table, epsilon, drunkenness=0.0):\n",
    "    \"\"\"\n",
    "    Selecciona la siguiente acción.\n",
    "    Prioridad:\n",
    "    1. Drunkenness (movimiento aleatorio involuntario/ruido).\n",
    "    2. Exploración (epsilon-greedy).\n",
    "    3. Explotación (mejor valor Q).\n",
    "    \"\"\"\n",
    "    # 1. Drunkenness\n",
    "    if drunkenness > 0.0 and np.random.random() < drunkenness:\n",
    "        return np.random.randint(4)\n",
    "    \n",
    "    # 2. Exploración\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "\n",
    "    # 3. Explotación\n",
    "    return np.argmax(q_table[row, col])\n",
    "\n",
    "def get_next_state(row, col, action_index, obstacles):\n",
    "    \"\"\"\n",
    "    Calcula el nuevo estado (row, col) dado el estado actual y una ACCIÓN específica.\n",
    "    Ya no decide la acción, solo ejecuta la física.\n",
    "    \"\"\"\n",
    "    action = ACTIONS[action_index]\n",
    "    new_row, new_col = row, col\n",
    "\n",
    "    if action == 'up' and row > 0:\n",
    "        new_row -= 1\n",
    "    elif action == 'down' and row < grid_rows - 1:\n",
    "        new_row += 1\n",
    "    elif action == 'left' and col > 0:\n",
    "        new_col -= 1\n",
    "    elif action == 'right' and col < grid_cols - 1:\n",
    "        new_col += 1\n",
    "\n",
    "    # Choque con obstáculos\n",
    "    if [new_row, new_col] in obstacles:\n",
    "        return row, col\n",
    "        \n",
    "    return new_row, new_col\n",
    "\n",
    "def get_shortest_path(start_row, start_col, q_table, grid_rewards, obstacles):\n",
    "    \"\"\"\n",
    "    Obtiene el camino más corto aprendida por la Q-Table desde un inicio hasta la meta.\n",
    "    Usa epsilon=0 para seguir la política óptima.\n",
    "    \"\"\"\n",
    "    if is_terminal_state(start_row, start_col, grid_rewards):\n",
    "        return []\n",
    "    \n",
    "    current_row, current_col = start_row, start_col\n",
    "    path = [[current_row, current_col]]\n",
    "    \n",
    "    while not is_terminal_state(current_row, current_col, grid_rewards):\n",
    "        # epsilon=0.0 para explotar la mejor ruta\n",
    "        action_index = get_next_action(current_row, current_col, q_table, epsilon=0.0, drunkenness=0.0)\n",
    "        current_row, current_col = get_next_state(\n",
    "            current_row, current_col, action_index, obstacles\n",
    "        )\n",
    "        path.append([current_row, current_col])\n",
    "        \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3602d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q_table, grid_rewards, obstacles, \n",
    "          alpha=ALPHA, gamma=GAMMA, \n",
    "          epsilon=EPSILON, epsilon_min=0.01, epsilon_decay=0.995,\n",
    "          conv_threshold=0.001, patience=10,\n",
    "          drunkenness=0.0):\n",
    "    \"\"\"\n",
    "    Entrena el agente usando Q-Learning.\n",
    "    \n",
    "    Args:\n",
    "        q_table: Tabla Q inicial.\n",
    "        grid_rewards: Matriz de recompensas del entorno.\n",
    "        obstacles: Lista de coordenadas de obstáculos.\n",
    "        alpha, gamma, epsilon: Hiperparámetros de Q-Learning.\n",
    "        drunkenness: Probabilidad de que el agente se mueva aleatoriamente (ruido).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Snapshots para análisis\n",
    "    snapshots = [q_table.copy()]\n",
    "    reward_history = []\n",
    "    \n",
    "    epoch_limit = 30000\n",
    "    total_epochs = 0\n",
    "    stable_epochs = 0\n",
    "    is_converged = False\n",
    "    \n",
    "    if drunkenness > 0.0:\n",
    "        print(f\"Iniciando entrenamiento (Drunkenness={drunkenness})...\")\n",
    "    else:\n",
    "        print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "    for epoch in trange(epoch_limit):\n",
    "        total_epochs += 1\n",
    "        \n",
    "        # Estado inicial fijo en (2,0)\n",
    "        current_row, current_col = 2, 0 \n",
    "        \n",
    "        total_reward = 0\n",
    "        old_q_table = q_table.copy()\n",
    "        \n",
    "        while not is_terminal_state(current_row, current_col, grid_rewards):\n",
    "            # Guardamos estado anterior para actualizar Q(s,a)\n",
    "            old_row, old_col = current_row, current_col\n",
    "            \n",
    "            # 1. Elegir Acción (Política + Drunkenness (si hubiere))\n",
    "            action_index = get_next_action(\n",
    "                current_row, current_col, \n",
    "                q_table, epsilon, drunkenness\n",
    "            )\n",
    "            \n",
    "            # 2. Ejecutar Movimiento\n",
    "            current_row, current_col = get_next_state(\n",
    "                current_row, current_col, \n",
    "                action_index, obstacles\n",
    "            )\n",
    "            \n",
    "            # 3. Obtener Recompensa\n",
    "            reward = grid_rewards[current_row, current_col]\n",
    "            \n",
    "            # 4. Actualizar Q-Value \n",
    "            # Q(s,a) = Q(s,a) + alpha * (R + gamma * max(Q(s',a')) - Q(s,a))\n",
    "            \n",
    "            old_q_value = q_table[old_row, old_col, action_index]\n",
    "            best_next_q = np.max(q_table[current_row, current_col])\n",
    "            \n",
    "            temporal_difference = reward + (gamma * best_next_q) - old_q_value\n",
    "            new_q_value = old_q_value + (alpha * temporal_difference)\n",
    "            \n",
    "            q_table[old_row, old_col, action_index] = new_q_value\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "        # Fin del epoch\n",
    "        \n",
    "        # Chequeo de Convergencia\n",
    "        delta = np.linalg.norm(q_table - old_q_table)\n",
    "        reward_history.append(total_reward)\n",
    "        \n",
    "        # Decay Epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Snapshots periódicos\n",
    "        snapshots.append(q_table.copy())\n",
    "        \n",
    "        if delta < conv_threshold:\n",
    "            stable_epochs += 1\n",
    "            if stable_epochs >= patience:\n",
    "                print(f\"Converged after {total_epochs} epochs.\")\n",
    "                is_converged = True\n",
    "                snapshots.append(q_table.copy()) # Guardar el convergido\n",
    "                break\n",
    "        else:\n",
    "            stable_epochs = 0\n",
    "            \n",
    "    # Asegurar snapshot final si no está\n",
    "    if len(snapshots) == 0 or not np.array_equal(q_table, snapshots[-1]):\n",
    "        snapshots.append(q_table.copy())\n",
    "        \n",
    "    # Selección de snapshots representativos para return\n",
    "    # (Inicio, 1/3, 2/3, Final) - aproximado\n",
    "    n_snaps = len(snapshots)\n",
    "    indices = [0, int(n_snaps*0.33), int(n_snaps*0.66), -1]\n",
    "    # Filtrar índices válidos y únicos (por si n_snaps es pequeño)\n",
    "    indices = sorted(list(set([i for i in indices if i < n_snaps])))\n",
    "    snapshots_output = [snapshots[i] for i in indices]\n",
    "    \n",
    "    # Métricas finales\n",
    "    avg_reward_final = np.mean(reward_history[-100:]) if len(reward_history) >= 100 else np.mean(reward_history)\n",
    "    avg_reward_total = np.mean(reward_history)\n",
    "    \n",
    "    return {\n",
    "        \"snapshots\": snapshots_output,\n",
    "        \"avg_reward_final\": avg_reward_final,\n",
    "        \"avg_reward_total\": avg_reward_total,\n",
    "        \"total_epochs\": total_epochs,\n",
    "        \"is_converged\": is_converged,\n",
    "        \"reward_history\": reward_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d7660c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Agente 1.a...\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 591/30000 [00:00<00:06, 4331.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 592 epochs.\n",
      "\n",
      "Entrenamiento finalizado en 592 épocas.\n",
      "¿Convergió?: True\n",
      "Recompensa Promedio Final: 95.72\n",
      "\n",
      "Camino Óptimo Aprendido (desde [2,0]):\n",
      "[[2, 0], [2, 1], [2, 2], [2, 3], [1, 3], [0, 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Ejecución Ejercicio 1.a (Entorno basico)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Reiniciamos la Q-Table para asegurar entrenamiento desde cero\n",
    "q_table = np.zeros((grid_rows, grid_cols, 4)) \n",
    "\n",
    "# 2. Entrenamos\n",
    "print(\"Entrenando Agente 1.a...\")\n",
    "results_1a = train(\n",
    "    q_table=q_table, \n",
    "    grid_rewards=rewards, \n",
    "    obstacles=obstacles, \n",
    "    alpha=ALPHA,\n",
    "    gamma=GAMMA,\n",
    "    epsilon=EPSILON,\n",
    "    conv_threshold=0.0001, \n",
    "    patience=20,\n",
    "    drunkenness=0.0 # Agente sobrio\n",
    ")\n",
    "\n",
    "# 3. Resultados\n",
    "print(f\"\\nEntrenamiento finalizado en {results_1a['total_epochs']} épocas.\")\n",
    "print(f\"¿Convergió?: {results_1a['is_converged']}\")\n",
    "print(f\"Recompensa Promedio Final: {results_1a['avg_reward_final']:.2f}\")\n",
    "\n",
    "# 4. Verificación del Camino Aprendido\n",
    "print(\"\\nCamino Óptimo Aprendido (desde [2,0]):\")\n",
    "path_1a = get_shortest_path(2, 0, q_table, rewards, obstacles)\n",
    "print(path_1a)\n",
    "\n",
    "# 5. Visualización de la Q-Table\n",
    "# display(q_table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abff24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configurando Entorno 1.b ===\n",
      "Recompensas 1.b:\n",
      "[[  -3.   -2.   -1.  100.]\n",
      " [  -4. -100.   -2.   -1.]\n",
      " [  -5.   -4.   -3.   -2.]]\n",
      "\n",
      "Entrenando Agente 1.b...\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 779/30000 [00:00<00:04, 6577.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 780 epochs.\n",
      "\n",
      "Entrenamiento 1.b finalizado en 780 épocas.\n",
      "¿Convergió?: True\n",
      "Recompensa Promedio Final: 89.34\n",
      "\n",
      "Camino Óptimo Aprendido 1.b (desde [2,0]):\n",
      "[[2, 0], [1, 0], [0, 0], [0, 1], [0, 2], [0, 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Ejecución Ejercicio 1.b (Entorno variado)\n",
    "# ==========================================\n",
    "\n",
    "print(\"=== Configurando Entorno 1.b ===\")\n",
    "\n",
    "# Definición del entorno 1.b\n",
    "rewards_b = np.array([\n",
    "    [-3., -2., -1., GOAL_SCORE],\n",
    "    [-4., -100., -2., -1.],  # Zona de peligro (le ponemos -100 pero en realidad se evita con obstacles)\n",
    "    [-5., -4., -3., -2.]\n",
    "])\n",
    "rewards_b[0, 3] = GOAL_SCORE # Asegurar meta\n",
    "obstacles_b = [[1, 1]]       # Mismo obstáculo\n",
    "\n",
    "print(\"Recompensas 1.b:\")\n",
    "print(rewards_b)\n",
    "\n",
    "# 1. Reiniciamos Q-Table para 1.b\n",
    "# Usamos grid_rows y grid_cols (minúsculas)\n",
    "q_table_b = np.zeros((grid_rows, grid_cols, 4))\n",
    "\n",
    "# 2. Entrenamos\n",
    "print(\"\\nEntrenando Agente 1.b...\")\n",
    "results_1b = train(\n",
    "    q_table=q_table_b, \n",
    "    grid_rewards=rewards_b, \n",
    "    obstacles=obstacles_b, \n",
    "    alpha=ALPHA,\n",
    "    gamma=GAMMA,\n",
    "    epsilon=EPSILON,\n",
    "    conv_threshold=0.0001, \n",
    "    patience=20,\n",
    "    drunkenness=0.0\n",
    ")\n",
    "\n",
    "# 3. Resultados\n",
    "print(f\"\\nEntrenamiento 1.b finalizado en {results_1b['total_epochs']} épocas.\")\n",
    "print(f\"¿Convergió?: {results_1b['is_converged']}\")\n",
    "print(f\"Recompensa Promedio Final: {results_1b['avg_reward_final']:.2f}\")\n",
    "\n",
    "# 4. Verificación del Camino Aprendido\n",
    "print(\"\\nCamino Óptimo Aprendido 1.b (desde [2,0]):\")\n",
    "path_1b = get_shortest_path(2, 0, q_table_b, rewards_b, obstacles_b)\n",
    "print(path_1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb2620",
   "metadata": {},
   "source": [
    "**Drunken sailor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Entrenando Drunken Sailor (1.c) ===\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 717/30000 [00:00<00:04, 5918.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 718 epochs.\n",
      "\n",
      "Entrenamiento (Drunkenness=0.01) finalizado en 718 épocas.\n",
      "¿Convergió?: True\n",
      "Recompensa Final: 89.46\n",
      "\n",
      "Camino aprendido por el agente borracho:\n",
      "[[2, 0], [1, 0], [0, 0], [0, 1], [0, 2], [0, 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Ejecución Ejercicio 1.c (Entorno variado despues de 10L de sangria)\n",
    "# ==========================================\n",
    "\n",
    "print(\"=== Entrenando Drunken Sailor (1.c) ===\")\n",
    "\n",
    "drunkenness_level = 0.01 # 1% de probabilidad de resbalar/acción aleatoria (99% de acción correcta)\n",
    "\n",
    "# 1. Configuración\n",
    "q_table_drunk = np.zeros((grid_rows, grid_cols, 4))\n",
    "\n",
    "# 2. Entrenamiento\n",
    "results_drunk = train(\n",
    "    q_table_drunk, \n",
    "    rewards_b, \n",
    "    obstacles_b, \n",
    "    conv_threshold=0.0001, \n",
    "    patience=20, \n",
    "    drunkenness=drunkenness_level\n",
    ")\n",
    "\n",
    "# 3. Resultados Individuales\n",
    "print(f\"\\nEntrenamiento (Drunkenness={drunkenness_level}) finalizado en {results_drunk['total_epochs']} épocas.\")\n",
    "print(f\"¿Convergió?: {results_drunk['is_converged']}\")\n",
    "print(f\"Recompensa Final: {results_drunk['avg_reward_final']:.2f}\")\n",
    "\n",
    "print(\"\\nCamino aprendido por el agente borracho:\")\n",
    "path_drunk = get_shortest_path(2, 0, q_table_drunk, rewards_b, obstacles_b)\n",
    "print(path_drunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956bda44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando comparativa (10 runs por configuración)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 738/30000 [00:00<00:05, 5293.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 739 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 779/30000 [00:00<00:05, 5061.94it/s]\n",
      "Runs:  10%|█         | 1/10 [00:00<00:02,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 780 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 776/30000 [00:00<00:05, 4937.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 777 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 776/30000 [00:00<00:05, 5797.16it/s]\n",
      "Runs:  20%|██        | 2/10 [00:00<00:02,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 777 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 635/30000 [00:00<00:05, 5640.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 636 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 619/30000 [00:00<00:06, 4511.73it/s]\n",
      "Runs:  30%|███       | 3/10 [00:00<00:02,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 620 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 579/30000 [00:00<00:05, 5291.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 580 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 614/30000 [00:00<00:07, 4109.46it/s]\n",
      "Runs:  40%|████      | 4/10 [00:01<00:01,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 615 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 629/30000 [00:00<00:06, 4821.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 630 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 817/30000 [00:00<00:06, 4420.46it/s]\n",
      "Runs:  50%|█████     | 5/10 [00:01<00:01,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 818 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 623/30000 [00:00<00:05, 4910.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 624 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 721/30000 [00:00<00:05, 4977.38it/s]\n",
      "Runs:  60%|██████    | 6/10 [00:01<00:01,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 722 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 722/30000 [00:00<00:06, 4735.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 723 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 621/30000 [00:00<00:07, 4146.77it/s]\n",
      "Runs:  70%|███████   | 7/10 [00:02<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 622 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 674/30000 [00:00<00:06, 4234.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 675 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 685/30000 [00:00<00:07, 3975.25it/s]\n",
      "Runs:  80%|████████  | 8/10 [00:02<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 686 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 579/30000 [00:00<00:06, 4270.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 580 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 681/30000 [00:00<00:06, 4374.15it/s]\n",
      "Runs:  90%|█████████ | 9/10 [00:02<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 682 epochs.\n",
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 656/30000 [00:00<00:05, 5069.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 657 epochs.\n",
      "Iniciando entrenamiento (Drunkenness=0.01)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 795/30000 [00:00<00:05, 5450.09it/s]\n",
      "Runs: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 796 epochs.\n",
      "\n",
      "========================================\n",
      "RESULTADOS (Promedio de 10 runs)\n",
      "========================================\n",
      "Sobrio (0.0):   662.1 epochs\n",
      "Borracho (0.1): 711.8 epochs\n",
      "----------------------------------------\n",
      "Impacto del Ruido: +7.5% de tiempo para converger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Comparación: Impacto del \"Drunkenness\"\n",
    "# ==========================================\n",
    "\n",
    "runs_comparison = 10 \n",
    "epochs_sober_list = []\n",
    "epochs_drunk_list = []\n",
    "\n",
    "print(f\"Iniciando comparativa ({runs_comparison} runs por configuración)...\")\n",
    "\n",
    "t = trange(runs_comparison, desc=\"Runs\")\n",
    "for _ in t:\n",
    "    # A. Normal/sobrio (Drunkenness = 0.0)\n",
    "    qt_sober = np.zeros((grid_rows, grid_cols, 4))\n",
    "    res_s = train(\n",
    "        qt_sober, rewards_b, obstacles_b,\n",
    "        alpha=0.1, gamma=0.99, epsilon=0.8,\n",
    "        conv_threshold=0.001, patience=20, drunkenness=0.0\n",
    "    )\n",
    "    if res_s['is_converged']:\n",
    "        epochs_sober_list.append(res_s['total_epochs'])\n",
    "        \n",
    "    # B. Borracho (Drunkenness = 0.01)\n",
    "    qt_drunk = np.zeros((grid_rows, grid_cols, 4))\n",
    "    res_d = train(\n",
    "        qt_drunk, rewards_b, obstacles_b, \n",
    "        alpha=0.1, gamma=0.99, epsilon=0.8,\n",
    "        conv_threshold=0.001, patience=20, drunkenness=0.01\n",
    "    )\n",
    "    if res_d['is_converged']:\n",
    "        epochs_drunk_list.append(res_d['total_epochs'])\n",
    "\n",
    "# Comparativa \n",
    "mean_sober = np.mean(epochs_sober_list)\n",
    "mean_drunk = np.mean(epochs_drunk_list)\n",
    "impact_pct = ((mean_drunk - mean_sober) / mean_sober) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"RESULTADOS (Promedio de {runs_comparison} runs)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Sobrio (0.0):   {mean_sober:.1f} epochs\")\n",
    "print(f\"Borracho (0.01): {mean_drunk:.1f} epochs\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Impacto de la borrachera: +{impact_pct:.1f}% de tiempo para converger\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
